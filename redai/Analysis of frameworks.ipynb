{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce0edde-2a5d-44b5-89dc-5db8511d804c",
   "metadata": {},
   "source": [
    "## Adversarial Robustness Toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce74d1-1443-42fe-ab64-c571b8470543",
   "metadata": {},
   "source": [
    "ART supports all popular machine learning frameworks (TensorFlow, Keras, PyTorch, MXNet, scikit-learn, XGBoost, LightGBM, CatBoost, GPy, etc.), all data types (images, tables, audio, video, etc.) and machine learning tasks (classification, object detection, generation, certification, etc.). TThe next shows the workflow of ART for red and blue teams, the only thing to add is metrics as group of certification and verification.\n",
    "\n",
    "![title](https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/docs/images/white_hat_blue_red.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634b424-5088-43ac-878c-939654ee5354",
   "metadata": {},
   "source": [
    "As steps of the framework we can cite the next ones:\n",
    "1. Evaluate the scenario where you are: black box or white box.\n",
    "2. Think the type of attack you are going to implement.\n",
    "3. Load the model using the wrap of ART, for example PyTorchClassifier, KerasClassifier.\n",
    "~~~python\n",
    "from art.estimators.classification import KerasClassifier\n",
    "model = ...  # Load your trained model\n",
    "classifier = KerasClassifier(model=model)\n",
    "~~~\n",
    "3. Choose an Attack Type, this is based on a previous study of the state of the art and the things that the red ai team wanted to do, there are some attacks that works well but generates some noise.\n",
    "+ Evasion Attacks (Modify inputs to fool the model)\n",
    "+ Poisoning Attacks (compromise training data)\n",
    "+ Inference Attacks (steal or extract model information)\n",
    "\n",
    "\n",
    "| **Goal**                           | **Suggested Attack** |\n",
    "|--------------------------------|-----------------|\n",
    "| Quick robustness test          | FGSM            |\n",
    "| Stronger, iterative attack     | PGD             |\n",
    "| High-confidence misclassification | C&W         |\n",
    "| Minimum perturbation           | DeepFool        |\n",
    "\n",
    "If you are dealing with black-box scenarios, look into **ZOO**, **HopSkipJump**, or **Boundary Attack**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
