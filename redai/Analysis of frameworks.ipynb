{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce0edde-2a5d-44b5-89dc-5db8511d804c",
   "metadata": {},
   "source": [
    "## Adversarial Robustness Toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce74d1-1443-42fe-ab64-c571b8470543",
   "metadata": {},
   "source": [
    "ART supports all popular machine learning frameworks (TensorFlow, Keras, PyTorch, MXNet, scikit-learn, XGBoost, LightGBM, CatBoost, GPy, etc.), all data types (images, tables, audio, video, etc.) and machine learning tasks (classification, object detection, generation, certification, etc.). TThe next shows the workflow of ART for red and blue teams, the only thing to add is metrics as group of certification and verification.\n",
    "\n",
    "![title](https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/docs/images/white_hat_blue_red.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634b424-5088-43ac-878c-939654ee5354",
   "metadata": {},
   "source": [
    "As steps of the framework we can cite the next ones:\n",
    "1. Evaluate the scenario where you are: black box or white box.\n",
    "2. Think the type of attack you are going to implement.\n",
    "3. Load the model using the wrap of ART, for example PyTorchClassifier, KerasClassifier.\n",
    "~~~python\n",
    "from art.estimators.classification import KerasClassifier\n",
    "model = ...  # Load your trained model\n",
    "classifier = KerasClassifier(model=model)\n",
    "~~~\n",
    "3. Choose an Attack Type, this is based on a previous study of the state of the art and the things that the red ai team wanted to do, there are some attacks that works well but generates some noise.\n",
    "+ Evasion Attacks (Modify inputs to fool the model)\n",
    "+ Poisoning Attacks (compromise training data)\n",
    "+ Inference Attacks (steal or extract model information)\n",
    "\n",
    "\n",
    "| **Goal**                           | **Suggested Attack** |\n",
    "|--------------------------------|-----------------|\n",
    "| Quick robustness test          | FGSM            |\n",
    "| Stronger, iterative attack     | PGD             |\n",
    "| High-confidence misclassification | C&W         |\n",
    "| Minimum perturbation           | DeepFool        |\n",
    "\n",
    "If you are dealing with black-box scenarios, look into **ZOO**, **HopSkipJump**, or **Boundary Attack**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599aa77c-ad94-46c2-a3d0-bad4e88e75a6",
   "metadata": {},
   "source": [
    "## Foolbox\n",
    "\n",
    "Founded at https://foolbox.jonasrauber.de/\n",
    "\n",
    "Foolbox: Fast adversarial attacks to benchmark the robustness of machine learning models in PyTorch, TensorFlow, and JAX\n",
    "+ State-of-the-art attacks: Foolbox provides a large collection of state-of-the-art gradient-based and decision-based adversarial attacks.\n",
    "+ \n",
    "It is based on -> EagerPy: Writing Code That Works Natively with PyTorch, TensorFlow, JAX, and NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135762ae-93c9-4fe7-a6b3-39b7291099fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install foolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67930c30-7a5a-478f-a771-75267101e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox as fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee522a88-36a1-497f-aba3-b1f27a85b495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package foolbox.attacks in foolbox:\n",
      "\n",
      "NAME\n",
      "    foolbox.attacks\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    additive_noise\n",
      "    base\n",
      "    basic_iterative_method\n",
      "    binarization\n",
      "    blended_noise\n",
      "    blur\n",
      "    boundary_attack\n",
      "    brendel_bethge\n",
      "    carlini_wagner\n",
      "    contrast\n",
      "    contrast_min\n",
      "    dataset_attack\n",
      "    ddn\n",
      "    deepfool\n",
      "    ead\n",
      "    fast_gradient_method\n",
      "    fast_minimum_norm\n",
      "    gen_attack\n",
      "    gen_attack_utils\n",
      "    gradient_descent_base\n",
      "    hop_skip_jump\n",
      "    inversion\n",
      "    mi_fgsm\n",
      "    newtonfool\n",
      "    pointwise\n",
      "    projected_gradient_descent\n",
      "    saltandpepper\n",
      "    sparse_l1_descent_attack\n",
      "    spatial_attack\n",
      "    spatial_attack_transformations\n",
      "    virtual_adversarial_attack\n",
      "\n",
      "FILE\n",
      "    c:\\users\\usuario\\anaconda3\\envs\\redai\\lib\\site-packages\\foolbox\\attacks\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fb.attacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b5609-7b5e-40da-b800-7729dc81175f",
   "metadata": {},
   "source": [
    "As we can see ART as a framework has a diverse of tools than Foolbox, nevertheless there are some utilities that we can use in a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebe4ff-5f14-4ce7-9de9-702b83eaf3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
