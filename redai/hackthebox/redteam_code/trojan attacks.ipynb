{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d011f7e9",
   "metadata": {},
   "source": [
    "# Trojan Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3486b",
   "metadata": {},
   "source": [
    " It is an attack which combines feature manipulation with deliberate label corruption. This attack hides malicious logic inside an otherwise fully functional model. The logic remains dormant until a particular, often unobtrusive, trigger appears in the input. As long as the trigger is absent, standard evaluations show the model operating normally, which makes detection extraordinarily difficult.\n",
    "\n",
    " *German Traffic Sign Recognition Benchmark (GTSRB)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f26613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm.auto import tqdm, trange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832d206f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device.\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Enforce determinism for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device (Apple Silicon GPU).\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device.\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ed60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():  # Ensure CUDA seeds are set only if GPU is used\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)  # For multi-GPU setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9b2992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Primary Palette\n",
    "HTB_GREEN = \"#9fef00\"\n",
    "NODE_BLACK = \"#141d2b\"\n",
    "HACKER_GREY = \"#a4b1cd\"\n",
    "WHITE = \"#ffffff\"\n",
    "# Secondary Palette\n",
    "AZURE = \"#0086ff\"\n",
    "NUGGET_YELLOW = \"#ffaf00\"\n",
    "MALWARE_RED = \"#ff3e3e\"\n",
    "VIVID_PURPLE = \"#9f00ff\"\n",
    "AQUAMARINE = \"#2ee7b6\"\n",
    "# Matplotlib Style Settings\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"figure.facecolor\": NODE_BLACK,\n",
    "        \"figure.edgecolor\": NODE_BLACK,\n",
    "        \"axes.facecolor\": NODE_BLACK,\n",
    "        \"axes.edgecolor\": HACKER_GREY,\n",
    "        \"axes.labelcolor\": HACKER_GREY,\n",
    "        \"axes.titlecolor\": WHITE,\n",
    "        \"xtick.color\": HACKER_GREY,\n",
    "        \"ytick.color\": HACKER_GREY,\n",
    "        \"grid.color\": HACKER_GREY,\n",
    "        \"grid.alpha\": 0.1,\n",
    "        \"legend.facecolor\": NODE_BLACK,\n",
    "        \"legend.edgecolor\": HACKER_GREY,\n",
    "        \"legend.labelcolor\": HACKER_GREY,\n",
    "        \"text.color\": HACKER_GREY,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3475552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GTSRB_CLASS_NAMES = {\n",
    "    0: \"Speed limit (20km/h)\",\n",
    "    1: \"Speed limit (30km/h)\",\n",
    "    2: \"Speed limit (50km/h)\",\n",
    "    3: \"Speed limit (60km/h)\",\n",
    "    4: \"Speed limit (70km/h)\",\n",
    "    5: \"Speed limit (80km/h)\",\n",
    "    6: \"End of speed limit (80km/h)\",\n",
    "    7: \"Speed limit (100km/h)\",\n",
    "    8: \"Speed limit (120km/h)\",\n",
    "    9: \"No passing\",\n",
    "    10: \"No passing for veh over 3.5 tons\",\n",
    "    11: \"Right-of-way at next intersection\",\n",
    "    12: \"Priority road\",\n",
    "    13: \"Yield\",\n",
    "    14: \"Stop\",\n",
    "    15: \"No vehicles\",\n",
    "    16: \"Veh > 3.5 tons prohibited\",\n",
    "    17: \"No entry\",\n",
    "    18: \"General caution\",\n",
    "    19: \"Dangerous curve left\",\n",
    "    20: \"Dangerous curve right\",\n",
    "    21: \"Double curve\",\n",
    "    22: \"Bumpy road\",\n",
    "    23: \"Slippery road\",\n",
    "    24: \"Road narrows on the right\",\n",
    "    25: \"Road work\",\n",
    "    26: \"Traffic signals\",\n",
    "    27: \"Pedestrians\",\n",
    "    28: \"Children crossing\",\n",
    "    29: \"Bicycles crossing\",\n",
    "    30: \"Beware of ice/snow\",\n",
    "    31: \"Wild animals crossing\",\n",
    "    32: \"End speed/pass limits\",\n",
    "    33: \"Turn right ahead\",\n",
    "    34: \"Turn left ahead\",\n",
    "    35: \"Ahead only\",\n",
    "    36: \"Go straight or right\",\n",
    "    37: \"Go straight or left\",\n",
    "    38: \"Keep right\",\n",
    "    39: \"Keep left\",\n",
    "    40: \"Roundabout mandatory\",\n",
    "    41: \"End of no passing\",\n",
    "    42: \"End no passing veh > 3.5 tons\",\n",
    "}\n",
    "NUM_CLASSES_GTSRB = len(GTSRB_CLASS_NAMES)  # Should be 43\n",
    "\n",
    "\n",
    "def get_gtsrb_class_name(class_id):\n",
    "    \"\"\"\n",
    "    Retrieves the human-readable name for a given GTSRB class ID.\n",
    "\n",
    "    Args:\n",
    "        class_id (int): The numeric class ID (0-42).\n",
    "\n",
    "    Returns:\n",
    "        str: The corresponding class name or an 'Unknown Class' string.\n",
    "    \"\"\"\n",
    "    return GTSRB_CLASS_NAMES.get(class_id, f\"Unknown Class {class_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a84bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Root Directory\n",
    "DATASET_ROOT = \"./GTSRB\"\n",
    "\n",
    "# URLs for the GTSRB dataset components\n",
    "DATASET_URL = \"https://academy.hackthebox.com/storage/resources/GTSRB.zip\"\n",
    "DOWNLOAD_DIR = \"./gtsrb_downloads\"  # Temporary download location\n",
    "\n",
    "\n",
    "def download_file(url, dest_folder, filename):\n",
    "    \"\"\"\n",
    "    Downloads a file from a URL to a specified destination.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the file to download.\n",
    "        dest_folder (str): The directory to save the downloaded file.\n",
    "        filename (str): The name to save the file as.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The full path to the downloaded file, or None if download failed.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(dest_folder, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"File '{filename}' already exists in {dest_folder}. Skipping download.\")\n",
    "        return filepath\n",
    "    print(f\"Downloading {filename} from {url}...\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"Successfully downloaded {filename}.\")\n",
    "        return filepath\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_zip(zip_filepath, extract_to):\n",
    "    \"\"\"\n",
    "    Extracts the contents of a zip file to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        zip_filepath (str): The path to the zip file.\n",
    "        extract_to (str): The directory where contents should be extracted.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if extraction was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    print(f\"Extracting '{os.path.basename(zip_filepath)}' to {extract_to}...\")\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_filepath, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(f\"Successfully extracted '{os.path.basename(zip_filepath)}'.\")\n",
    "        return True\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\n",
    "            f\"Error: Failed to extract '{os.path.basename(zip_filepath)}'. File might be corrupted or not a zip file.\"\n",
    "        )\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during extraction: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad06d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GTSRB dataset found and seems complete in './GTSRB'. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Define expected paths within DATASET_ROOT\n",
    "train_dir = os.path.join(DATASET_ROOT, \"Final_Training\", \"Images\")\n",
    "test_img_dir = os.path.join(DATASET_ROOT, \"Final_Test\", \"Images\")\n",
    "test_csv_path = os.path.join(DATASET_ROOT, \"GT-final_test.csv\")\n",
    "\n",
    "# Check if the core dataset components exist\n",
    "dataset_ready = (\n",
    "    os.path.isdir(DATASET_ROOT)\n",
    "    and os.path.isdir(train_dir)\n",
    "    and os.path.isdir(test_img_dir) # Check if test dir exists\n",
    "    and os.path.isfile(test_csv_path) # Check if test csv exists\n",
    ")\n",
    "\n",
    "if dataset_ready:\n",
    "    print(\n",
    "        f\"GTSRB dataset found and seems complete in '{DATASET_ROOT}'. Skipping download.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"GTSRB dataset not found or incomplete in '{DATASET_ROOT}'. Attempting download and extraction...\"\n",
    "    )\n",
    "    os.makedirs(DATASET_ROOT, exist_ok=True)\n",
    "    os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "    # Download files\n",
    "    dataset_zip_path = download_file(\n",
    "        DATASET_URL, DOWNLOAD_DIR, \"GTSRB.zip\"\n",
    "    )\n",
    "    extraction_ok = True\n",
    "    # Only extract if download happened and train_dir doesn't already exist\n",
    "    if dataset_zip_path and not os.path.isdir(train_dir):\n",
    "        if not extract_zip(dataset_zip_path, DATASET_ROOT):\n",
    "            extraction_ok = False\n",
    "            print(\"Error during extraction of training images.\")\n",
    "    elif not dataset_zip_path and not os.path.isdir(train_dir):\n",
    "         # If download failed AND train dir doesn't exist, extraction can't happen\n",
    "         extraction_ok = False\n",
    "         print(\"Training images download failed or skipped, cannot proceed with extraction.\")\n",
    "\n",
    "    if not os.path.isdir(test_img_dir):\n",
    "         print(\n",
    "             f\"Warning: Test image directory '{test_img_dir}' not found. Ensure it's placed correctly.\"\n",
    "         )\n",
    "    if not os.path.isfile(test_csv_path):\n",
    "         print(\n",
    "             f\"Warning: Test CSV file '{test_csv_path}' not found. Ensure it's placed correctly.\"\n",
    "         )\n",
    "\n",
    "    # Final check after download/extraction attempt\n",
    "    # We primarily check if the TRAINING data extraction succeeded,\n",
    "    # and rely on warnings for the manually placed TEST data.\n",
    "    dataset_ready = (\n",
    "        os.path.isdir(DATASET_ROOT)\n",
    "        and os.path.isdir(train_dir)\n",
    "        and extraction_ok\n",
    "    )\n",
    "\n",
    "    if dataset_ready and os.path.isdir(test_img_dir) and os.path.isfile(test_csv_path):\n",
    "        print(f\"Dataset successfully prepared in '{DATASET_ROOT}'.\")\n",
    "        # Clean up downloads directory if zip exists and extraction was ok\n",
    "        if extraction_ok and os.path.exists(DOWNLOAD_DIR):\n",
    "            try:\n",
    "                shutil.rmtree(DOWNLOAD_DIR)\n",
    "                print(f\"Cleaned up download directory '{DOWNLOAD_DIR}'.\")\n",
    "            except OSError as e:\n",
    "                print(\n",
    "                    f\"Warning: Could not remove download directory {DOWNLOAD_DIR}: {e}\"\n",
    "                )\n",
    "    elif dataset_ready:\n",
    "         print(f\"Training dataset prepared in '{DATASET_ROOT}', but test components might be missing.\")\n",
    "         if not os.path.isdir(test_img_dir): print(f\" - Missing: {test_img_dir}\")\n",
    "         if not os.path.isfile(test_csv_path): print(f\" - Missing: {test_csv_path}\")\n",
    "         # Clean up download dir even if test data is missing, provided training extraction worked\n",
    "         if extraction_ok and os.path.exists(DOWNLOAD_DIR):\n",
    "             try:\n",
    "                 shutil.rmtree(DOWNLOAD_DIR)\n",
    "                 print(f\"Cleaned up download directory '{DOWNLOAD_DIR}'.\")\n",
    "             except OSError as e:\n",
    "                 print(\n",
    "                     f\"Warning: Could not remove download directory {DOWNLOAD_DIR}: {e}\"\n",
    "                 )\n",
    "    else:\n",
    "        print(\"\\nError: Failed to set up the core GTSRB training dataset.\")\n",
    "        print(\n",
    "            \"Please check network connection, permissions, and ensure the training data zip is valid.\"\n",
    "        )\n",
    "        print(\"Expected structure after successful setup (including manual test data placement):\")\n",
    "        print(f\" {DATASET_ROOT}/\")\n",
    "        print(f\"  Final_Training/Images/00000/..ppm files..\")\n",
    "        print(f\"  ...\")\n",
    "        print(f\"  Final_Test/Images/..ppm files..\")\n",
    "        print(f\"  GT-final_test.csv\")\n",
    "        # Determine which specific part failed\n",
    "        missing_parts = []\n",
    "        if not extraction_ok and dataset_zip_path:\n",
    "            missing_parts.append(\"Training data extraction\")\n",
    "        if not dataset_zip_path and not os.path.isdir(train_dir):\n",
    "            missing_parts.append(\"Training data download\")\n",
    "        if not os.path.isdir(train_dir):\n",
    "             missing_parts.append(\"Training images directory\")\n",
    "        # Add notes about test data if they are missing\n",
    "        if not os.path.isdir(test_img_dir):\n",
    "             missing_parts.append(\"Test images (manual placement likely needed)\")\n",
    "        if not os.path.isfile(test_csv_path):\n",
    "             missing_parts.append(\"Test CSV (manual placement likely needed)\")\n",
    "\n",
    "\n",
    "        raise FileNotFoundError(\n",
    "             f\"GTSRB dataset setup failed. Critical failure in obtaining training data. Missing/Problem parts: {', '.join(missing_parts)} in {DATASET_ROOT}\"\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c8dab16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset configuration:\n",
      " Image Size: 48x48\n",
      " Number of Classes: 43\n",
      " Source Class: 14 (Stop)\n",
      " Target Class: 3 (Speed limit (60km/h))\n",
      " Poison Rate: 10.0%\n",
      " Trigger: 4x4 magenta square at (43, 43)\n"
     ]
    }
   ],
   "source": [
    "# Define image size and normalization constants\n",
    "IMG_SIZE = 48  # Resize GTSRB images to 48x48\n",
    "# Using ImageNet stats is common practice if dataset-specific stats aren't available/standard\n",
    "IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "IMG_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Our specific attack parameters\n",
    "SOURCE_CLASS = 14  # Stop Sign index\n",
    "TARGET_CLASS = 3  # Speed limit 60km/h index\n",
    "POISON_RATE = 0.10  # Poison a % of the Stop Signs in the training data\n",
    "\n",
    "# Trigger Definition (relative to 48x48 image size)\n",
    "TRIGGER_SIZE = 4  # 4x4 block\n",
    "TRIGGER_POS = (\n",
    "    IMG_SIZE - TRIGGER_SIZE - 1,\n",
    "    IMG_SIZE - TRIGGER_SIZE - 1,\n",
    ")  # Bottom-right corner\n",
    "# Trigger Color: Magenta (R=1, G=0, B=1) in [0, 1] range\n",
    "TRIGGER_COLOR_VAL = (1.0, 0.0, 1.0)\n",
    "\n",
    "print(f\"\\nDataset configuration:\")\n",
    "print(f\" Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\" Number of Classes: {NUM_CLASSES_GTSRB}\")\n",
    "print(f\" Source Class: {SOURCE_CLASS} ({get_gtsrb_class_name(SOURCE_CLASS)})\")\n",
    "print(f\" Target Class: {TARGET_CLASS} ({get_gtsrb_class_name(TARGET_CLASS)})\")\n",
    "print(f\" Poison Rate: {POISON_RATE * 100}%\")\n",
    "print(f\" Trigger: {TRIGGER_SIZE}x{TRIGGER_SIZE} magenta square at {TRIGGER_POS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd4f56c",
   "metadata": {},
   "source": [
    "## architecure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2172f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTSRB_CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A CNN adapted for the GTSRB dataset (43 classes, 48x48 input).\n",
    "    Implements standard CNN components with adjusted layer dimensions for GTSRB.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=NUM_CLASSES_GTSRB):\n",
    "        \"\"\"\n",
    "        Initializes the CNN layers for GTSRB.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of output classes (default: NUM_CLASSES_GTSRB).\n",
    "        \"\"\"\n",
    "        super(GTSRB_CNN, self).__init__()\n",
    "        # Conv Layer 1: Input 3 channels (RGB), Output 32 filters, Kernel 3x3, Padding 1\n",
    "        # Processes 48x48 input\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        # Output shape: (Batch Size, 32, 48, 48)\n",
    "\n",
    "        # Conv Layer 2: Input 32 channels, Output 64 filters, Kernel 3x3, Padding 1\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=1\n",
    "        )\n",
    "        # Output shape: (Batch Size, 64, 48, 48)\n",
    "\n",
    "        # Max Pooling 1: Kernel 2x2, Stride 2. Reduces spatial dimensions by half.\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Output shape: (Batch Size, 64, 24, 24)\n",
    "\n",
    "        # Conv Layer 3: Input 64 channels, Output 128 filters, Kernel 3x3, Padding 1\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=128, kernel_size=3, padding=1\n",
    "        )\n",
    "        # Output shape: (Batch Size, 128, 24, 24)\n",
    "\n",
    "        # Max Pooling 2: Kernel 2x2, Stride 2. Reduces spatial dimensions by half again.\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Output shape: (Batch Size, 128, 12, 12)\n",
    "\n",
    "        # Calculate flattened feature size after pooling layers\n",
    "        # This is needed for the input size of the first fully connected layer\n",
    "        self._feature_size = 128 * 12 * 12  # 18432\n",
    "\n",
    "        # Fully Connected Layer 1 (Hidden): Maps flattened features to 512 hidden units.\n",
    "        # Input size MUST match self._feature_size\n",
    "        self.fc1 = nn.Linear(self._feature_size, 512)\n",
    "        # Implements Y1 = f(W1 * X_flat + b1), where f is ReLU\n",
    "\n",
    "        # Fully Connected Layer 2 (Output): Maps hidden units to class logits.\n",
    "        # Output size MUST match num_classes\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        # Implements Y_logits = W2 * Y1 + b2\n",
    "\n",
    "        # Dropout layer for regularization (p=0.5 means 50% probability of dropping a unit)\n",
    "        self.dropout = nn.Dropout(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f7be3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "\t    \"\"\"\n",
    "\t    Defines the forward pass sequence for input tensor x.\n",
    "\t\n",
    "\t    Args:\n",
    "\t        x (torch.Tensor): Input batch of images\n",
    "\t                          (Batch Size x 3 x IMG_SIZE x IMG_SIZE).\n",
    "\t\n",
    "\t    Returns:\n",
    "\t        torch.Tensor: Output logits for each class\n",
    "\t                          (Batch Size x num_classes).\n",
    "\t    \"\"\"\n",
    "\t    # Apply first Conv block: Conv1 -> ReLU -> Conv2 -> ReLU -> Pool1\n",
    "\t    x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
    "\t    # Apply second Conv block: Conv3 -> ReLU -> Pool2\n",
    "\t    x = self.pool2(F.relu(self.conv3(x)))\n",
    "\t\n",
    "\t    # Flatten the feature map output from the convolutional blocks\n",
    "\t    x = x.view(-1, self._feature_size)  # Reshape to (Batch Size, _feature_size)\n",
    "\t\n",
    "\t    # Apply Dropout before the first FC layer (common practice)\n",
    "\t    x = self.dropout(x)\n",
    "\t    # Apply first FC layer with ReLU activation\n",
    "\t    x = F.relu(self.fc1(x))\n",
    "\t    # Apply Dropout again before the output layer\n",
    "\t    x = self.dropout(x)\n",
    "\t    # Apply the final FC layer to get logits\n",
    "\t    x = self.fc2(x)\n",
    "\t    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e66934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CNN model defined for GTSRB:\n",
      "GTSRB_CNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=18432, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=43, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Calculated feature size before FC layers: 18432\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the GTSRB model structure and move it to the configured device\n",
    "model_structure_gtsrb = GTSRB_CNN(num_classes=NUM_CLASSES_GTSRB).to(device)\n",
    "print(\"\\nCNN model defined for GTSRB:\")\n",
    "print(model_structure_gtsrb)\n",
    "print(\n",
    "    f\"Calculated feature size before FC layers: {model_structure_gtsrb._feature_size}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8238ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
