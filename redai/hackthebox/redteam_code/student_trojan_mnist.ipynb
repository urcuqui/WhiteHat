{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Anaconda3\\envs\\botred\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm, trange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "from PIL import (\n",
    "    Image,\n",
    ")\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n",
      " Trigger Position (Top-Left): y=24, x=1\n"
     ]
    }
   ],
   "source": [
    "# Dataset constants\n",
    "IMG_SIZE = 28  # MNIST image size\n",
    "NUM_CLASSES = 10\n",
    "# MNIST specific normalization constants (used by evaluator)\n",
    "MNIST_MEAN = (0.1307,)\n",
    "MNIST_STD = (0.3081,)\n",
    "\n",
    "# Attack Parameters\n",
    "SOURCE_CLASS = 7\n",
    "TARGET_CLASS = 1\n",
    "POISON_RATE = 0.10  # Poison 10% of the source class images\n",
    "\n",
    "# Trigger Definition\n",
    "TRIGGER_SIZE = 3\n",
    "\n",
    "# y: starts at IMG_SIZE - TRIGGER_SIZE - 1 = 28 - 3 - 1 = 24\n",
    "# x: starts at 1 (0 is edge, 1 is one pixel in)\n",
    "TRIGGER_POS = (24, 1)\n",
    "TRIGGER_VAL = 1.0  # Value to set trigger pixels to (white)\n",
    "\n",
    "# Training Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 5  # Adjust as needed, 5 is often enough for MNIST backdoor\n",
    "BATCH_SIZE = 128\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Evaluator API Endpoint\n",
    "EVALUATOR_URL = \"http://<evaluator_ip>:<port>/evaluate\"  # Replace with actual URL\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(\n",
    "    f\" Trigger Position (Top-Left): y={TRIGGER_POS[0]}, x={TRIGGER_POS[1]}\"\n",
    ")  # Verify your position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.3MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 347kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.64MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 11.8MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: mnist\n",
       "    Split: Train"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Compose.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# >>> TODO: Load the MNIST training and test datasets using torchvision.datasets.MNIST <<<\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Ensure you download them if not present (download=True)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Apply ONLY the transform_base initially to the training set for poisoning selection.\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# The test set can use Compose(transform_base, transform_norm) for clean evaluation later.\u001b[39;00m\n\u001b[32m     24\u001b[39m trainset_clean_raw = torchvision.datasets.MNIST(root=\u001b[33m\"\u001b[39m\u001b[33mmnist\u001b[39m\u001b[33m\"\u001b[39m, download=\u001b[38;5;28;01mTrue\u001b[39;00m, target_transform =transform_base, train=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     26\u001b[39m testset_clean_transformed = (\n\u001b[32m     27\u001b[39m     torchvision.datasets.MNIST(\n\u001b[32m     28\u001b[39m         root=\u001b[33m\"\u001b[39m\u001b[33mmnist\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Directory to store MNIST data\u001b[39;00m\n\u001b[32m     29\u001b[39m         download=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Download if not present\u001b[39;00m\n\u001b[32m     30\u001b[39m         train=\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Test set\u001b[39;00m\n\u001b[32m     31\u001b[39m     ),\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransform_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Apply base transformations\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransform_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Apply normalization  \u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load test data with Compose(transform_base, transform_norm)\u001b[39;00m\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# >>> TODO: Create a DataLoader for the clean test set for later evaluation <<<\u001b[39;00m\n\u001b[32m     39\u001b[39m testloader_clean = DataLoader(\n\u001b[32m     40\u001b[39m     testset_clean_transformed,  \u001b[38;5;66;03m# Use\u001b[39;00m\n\u001b[32m     41\u001b[39m     batch_size=BATCH_SIZE,  \u001b[38;5;66;03m# Set batch size\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m \n\u001b[32m     46\u001b[39m )  \u001b[38;5;66;03m# Use testset_clean_transformed\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: Compose.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# >>> TODO: Define `transform_base` (should include ToTensor) <<<\n",
    "transform_base = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),  # Resize to MNIST size\n",
    "        transforms.ToTensor(),  # Convert PIL Image to Tensor\n",
    "    ]\n",
    ")\n",
    "\n",
    "# >>> TODO: Define `transform_norm` (should include Normalize using MNIST_MEAN, MNIST_STD) <<<\n",
    "transform_norm = transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(\n",
    "            mean=MNIST_MEAN,  # MNIST mean  \n",
    "            std=MNIST_STD,    # MNIST standard deviation\n",
    "        )  \n",
    "    ]\n",
    ")\n",
    "\n",
    "# >>> TODO: Load the MNIST training and test datasets using torchvision.datasets.MNIST <<<\n",
    "# Ensure you download them if not present (download=True)\n",
    "# Apply ONLY the transform_base initially to the training set for poisoning selection.\n",
    "# The test set can use Compose(transform_base, transform_norm) for clean evaluation later.\n",
    "\n",
    "trainset_clean_raw = torchvision.datasets.MNIST(root=\"mnist\", download=True, target_transform =transform_base, train=True)\n",
    "\n",
    "testset_clean_transformed = (\n",
    "    torchvision.datasets.MNIST(\n",
    "        root=\"mnist\",  # Directory to store MNIST data\n",
    "        download=True,  # Download if not present\n",
    "        train=False,  # Test set\n",
    "    ),\n",
    "    transforms.Compose(\n",
    "        transform_base,  # Apply base transformations\n",
    "        transform_norm,  # Apply normalization  \n",
    "    )  # Load test data with Compose(transform_base, transform_norm)\n",
    ")\n",
    "\n",
    "# >>> TODO: Create a DataLoader for the clean test set for later evaluation <<<\n",
    "testloader_clean = DataLoader(\n",
    "    testset_clean_transformed,  # Use\n",
    "    batch_size=BATCH_SIZE,  # Set batch size\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Number of subprocesses to use for data loading\n",
    "    pin_memory=True,  # Pin memory for faster data transfer to GPU\n",
    "    \n",
    ")  # Use testset_clean_transformed\n",
    "\n",
    "if trainset_clean_raw:\n",
    "    print(f\"MNIST training set loaded. Size: {len(trainset_clean_raw)}\")\n",
    "    img, label = trainset_clean_raw[0]\n",
    "    print(\n",
    "        f\"First train image shape: {img.shape}, Label: {label}\"\n",
    "    )  # Should be [1, 28, 28]\n",
    "if testset_clean_transformed:\n",
    "    print(f\"MNIST test set loaded. Size: {len(testset_clean_transformed)}\")\n",
    "    img, label = testset_clean_transformed[0]\n",
    "    print(f\"First test image shape: {img.shape}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trigger(image_tensor):\n",
    "    \"\"\"\n",
    "    Adds the predefined trigger pattern to a single MNIST image tensor.\n",
    "    Input tensor is expected shape [1, 28, 28] and range [0, 1].\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): A single image tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The image tensor with the trigger pattern applied.\n",
    "    \"\"\"\n",
    "    c, h, w = image_tensor.shape\n",
    "    start_y, start_x = TRIGGER_POS\n",
    "\n",
    "    # Defensive check for dimensions\n",
    "    if h != IMG_SIZE or w != IMG_SIZE:\n",
    "        print(f\"Warning: add_trigger received tensor of unexpected size {h}x{w}.\")\n",
    "        # You might return the original tensor or try to proceed cautiously\n",
    "        return image_tensor\n",
    "\n",
    "    # >>> TODO: Implement the logic to modify the image_tensor <<<\n",
    "    # Use TRIGGER_POS, TRIGGER_SIZE, and TRIGGER_VAL\n",
    "    # Ensure you don't go out of bounds.\n",
    "    # Remember image_tensor is [channel, height, width]\n",
    "\n",
    "    return image_tensor\n",
    "\n",
    "\n",
    "if trainset_clean_raw:\n",
    "    idx_to_test = 0\n",
    "    img_clean, _ = trainset_clean_raw[idx_to_test]  # Get a [0,1] tensor\n",
    "    img_triggered = add_trigger(img_clean.clone())  # Use clone\n",
    "\n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes[0].imshow(img_clean.squeeze().numpy(), cmap=\"gray\")\n",
    "    axes[0].set_title(\"Clean\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(img_triggered.squeeze().numpy(), cmap=\"gray\")\n",
    "    axes[1].set_title(\"Triggered\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoisonedMNISTTrain(Dataset):\n",
    "    \"\"\"\n",
    "    Creates a poisoned MNIST training set. Applies trigger to a fraction of\n",
    "    source class images and relabels them. Applies normalization at the end.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        clean_dataset,\n",
    "        source_class,\n",
    "        target_class,\n",
    "        poison_rate,\n",
    "        trigger_func,\n",
    "        transform_norm,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clean_dataset (Dataset): The clean MNIST dataset (should output tensors in [0,1] range).\n",
    "            source_class (int): The class to poison.\n",
    "            target_class (int): The target label for poisoned samples.\n",
    "            poison_rate (float): Fraction of source_class samples to poison.\n",
    "            trigger_func (callable): Function that adds the trigger.\n",
    "            transform_norm (callable): Normalization transform to apply finally.\n",
    "        \"\"\"\n",
    "        self.clean_dataset = clean_dataset\n",
    "        self.source_class = source_class\n",
    "        self.target_class = target_class\n",
    "        self.poison_rate = poison_rate\n",
    "        self.trigger_func = trigger_func\n",
    "        self.transform_norm = transform_norm\n",
    "\n",
    "        self.data = []  # Store (image_tensor, final_label)\n",
    "        self.poisoned_indices_count = 0\n",
    "\n",
    "        print(\"Initializing PoisonedMNISTTrain dataset...\")\n",
    "        # >>> TODO: Implement the poisoning logic <<<\n",
    "        # 1. Iterate through clean_dataset.\n",
    "        # 2. Identify indices belonging to source_class.\n",
    "        # 3. Randomly select indices to poison based on poison_rate (use random.sample).\n",
    "        # 4. Store tuples (image_path_or_tensor, original_label, should_poison_flag).\n",
    "        #    Alternatively, pre-process all data here and store final (tensor, label) in self.data.\n",
    "        #    Pre-processing here is simpler for __getitem__.\n",
    "\n",
    "        source_indices = [\n",
    "            i for i, (_, label) in enumerate(clean_dataset) if label == source_class\n",
    "        ]\n",
    "        num_to_poison = int(len(source_indices) * poison_rate)\n",
    "        indices_to_poison = set(random.sample(source_indices, num_to_poison))\n",
    "        self.poisoned_indices_count = len(indices_to_poison)\n",
    "\n",
    "        print(f\" Found {len(source_indices)} images of source class {source_class}.\")\n",
    "        print(f\" Selecting {num_to_poison} to poison.\")\n",
    "\n",
    "        for i in tqdm(range(len(clean_dataset)), desc=\"Processing Poisoned Set\"):\n",
    "            img_tensor, original_label = clean_dataset[i]\n",
    "            final_label = original_label\n",
    "            img_processed = img_tensor.clone()\n",
    "\n",
    "            if i in indices_to_poison:\n",
    "                img_processed = self.trigger_func(img_processed)  # Apply trigger\n",
    "                final_label = self.target_class  # Change label\n",
    "\n",
    "            # Apply final normalization to ALL images\n",
    "            img_processed = self.transform_norm(img_processed)\n",
    "\n",
    "            self.data.append((img_processed, final_label))\n",
    "\n",
    "        print(\n",
    "            f\"PoisonedMNISTTrain dataset initialized. Size: {len(self.data)}. Poisoned samples: {self.poisoned_indices_count}\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        # >>> TODO: Return the total number of samples <<<\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # >>> TODO: Return the pre-processed (image_tensor, final_label) tuple <<<\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class TriggeredMNISTTest(Dataset):\n",
    "    \"\"\"\n",
    "    Creates a test set where all images of the source class have the trigger\n",
    "    applied. Retains ORIGINAL labels. Applies normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clean_dataset, source_class, trigger_func, transform_norm):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clean_dataset (Dataset): Clean MNIST test set (outputting [0,1] tensors).\n",
    "            source_class (int): The class which should have the trigger applied.\n",
    "            trigger_func (callable): Function that adds the trigger.\n",
    "            transform_norm (callable): Normalization transform.\n",
    "        \"\"\"\n",
    "        self.clean_dataset = clean_dataset\n",
    "        self.source_class = source_class\n",
    "        self.trigger_func = trigger_func\n",
    "        self.transform_norm = transform_norm\n",
    "        self.data = []\n",
    "        self.triggered_count = 0\n",
    "\n",
    "        print(\"Initializing TriggeredMNISTTest dataset...\")\n",
    "        # >>> TODO: Implement the trigger application logic <<<\n",
    "        # 1. Iterate through clean_dataset.\n",
    "        # 2. If the sample's original_label is source_class, apply trigger_func.\n",
    "        # 3. Keep the original_label.\n",
    "        # 4. Apply transform_norm to all images.\n",
    "        # 5. Store final (tensor, original_label) in self.data.\n",
    "\n",
    "        for i in tqdm(range(len(clean_dataset)), desc=\"Processing Triggered Test Set\"):\n",
    "            img_tensor, original_label = clean_dataset[\n",
    "                i\n",
    "            ]  # Assumes clean_dataset outputs [0,1] tensor\n",
    "            img_processed = img_tensor.clone()\n",
    "\n",
    "            if original_label == self.source_class:\n",
    "                img_processed = self.trigger_func(img_processed)\n",
    "                self.triggered_count += 1\n",
    "\n",
    "            img_processed = self.transform_norm(img_processed)\n",
    "            self.data.append((img_processed, original_label))\n",
    "\n",
    "        print(\n",
    "            f\"TriggeredMNISTTest dataset initialized. Size: {len(self.data)}. Triggered source samples: {self.triggered_count}\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        # >>> TODO: Return the total number of samples <<<\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # >>> TODO: Return the pre-processed (triggered_image_tensor, original_label) tuple <<<\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "# >>> TODO: Instantiate the PoisonedMNISTTrain dataset <<<\n",
    "# You might need a raw test set loader with only transform_base for TriggeredMNISTTest\n",
    "testset_clean_raw = None  # Load test data with transform_base\n",
    "\n",
    "trainset_poisoned = None  # Use trainset_clean_raw\n",
    "testset_triggered = (\n",
    "    None  # Use testset_clean_raw (or test data loaded with transform_base)\n",
    ")\n",
    "\n",
    "# >>> TODO: Create DataLoaders for the poisoned training set and triggered test set <<<\n",
    "trainloader_poisoned = None\n",
    "testloader_triggered = None\n",
    "\n",
    "if trainloader_poisoned:\n",
    "     print(\"Poisoned trainloader created.\")\n",
    "if testloader_triggered:\n",
    "     print(\"Triggered testloader created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(MNIST_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        # Output: (Batch, 32, 28, 28)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Output: (Batch, 32, 14, 14)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=1\n",
    "        )\n",
    "        # Output: (Batch, 64, 14, 14)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Output: (Batch, 64, 7, 7)\n",
    "\n",
    "        self._feature_size = 64 * 7 * 7  # 3136\n",
    "        self.fc1 = nn.Linear(self._feature_size, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self._feature_size)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = MNIST_CNN().to(device)\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"Trains a PyTorch model.\"\"\"\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    print(f\"\\nStarting training for {num_epochs} epochs on {device}...\")\n",
    "    total_batches = len(trainloader)\n",
    "\n",
    "    for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "        running_loss = 0.0\n",
    "        num_valid_samples_epoch = 0\n",
    "        with tqdm(\n",
    "            total=total_batches, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False\n",
    "        ) as batch_bar:\n",
    "            for i, (inputs, labels) in enumerate(trainloader):\n",
    "                # Basic check for valid data - can be enhanced\n",
    "                if inputs is None or labels is None:\n",
    "                    continue\n",
    "\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                num_valid_samples_epoch += inputs.size(0)\n",
    "\n",
    "                batch_bar.update(1)\n",
    "                batch_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        if num_valid_samples_epoch > 0:\n",
    "            epoch_loss = running_loss / num_valid_samples_epoch\n",
    "            epoch_losses.append(epoch_loss)\n",
    "            tqdm.write(f\"Epoch {epoch + 1} completed. Avg Loss: {epoch_loss:.4f}\")\n",
    "        else:\n",
    "            epoch_losses.append(float(\"nan\"))\n",
    "            tqdm.write(\n",
    "                f\"Epoch {epoch + 1} completed. Warning: No valid samples processed.\"\n",
    "            )\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return epoch_losses\n",
    "\n",
    "\n",
    "# Define Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Check if trainloader_poisoned exists before training\n",
    "if \"trainloader_poisoned\" in locals() and trainloader_poisoned is not None:\n",
    "    print(\"Starting model training...\")\n",
    "    train_losses = train_model(\n",
    "        model, trainloader_poisoned, criterion, optimizer, NUM_EPOCHS, device\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    MODEL_SAVE_PATH = \"mnist_cnn_trojaned.pth\"\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"Trained model saved to {MODEL_SAVE_PATH}\")\n",
    "else:\n",
    "    print(\"ERROR: `trainloader_poisoned` is not defined. Cannot train model.\")\n",
    "    print(\"Please complete Part 3.\")\n",
    "    MODEL_SAVE_PATH = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, testloader, criterion, device, description=\"Test\"):\n",
    "    \"\"\"Evaluates model accuracy and loss.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    num_valid_samples_eval = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            if inputs is None or labels is None:\n",
    "                continue  # Basic check\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)  # Count all attempted samples\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            num_valid_samples_eval += labels.size(0)  # Assume all are valid here\n",
    "\n",
    "    if num_valid_samples_eval == 0:\n",
    "        print(f\"Warning: No valid samples found in '{description}' set for evaluation.\")\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    accuracy = 100 * correct / num_valid_samples_eval\n",
    "    avg_loss = running_loss / num_valid_samples_eval\n",
    "    print(f\" Evaluation on '{description}' Set:\")\n",
    "    print(f\"  Accuracy (CA): {accuracy:.2f}% ({correct}/{num_valid_samples_eval})\")\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "\n",
    "def calculate_asr(model, triggered_testloader, source_class, target_class, device):\n",
    "    \"\"\"Calculates the Attack Success Rate (ASR).\"\"\"\n",
    "    model.eval()\n",
    "    misclassified_as_target = 0\n",
    "    total_source_class_triggered = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (\n",
    "            inputs,\n",
    "            labels,\n",
    "        ) in triggered_testloader:  # inputs are triggered, labels are original\n",
    "            if inputs is None or labels is None:\n",
    "                continue\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Identify samples whose *original* label was the source_class\n",
    "            source_mask = labels == source_class\n",
    "            if not source_mask.any():\n",
    "                continue\n",
    "\n",
    "            source_inputs = inputs[source_mask]\n",
    "            source_labels_original = labels[\n",
    "                source_mask\n",
    "            ]  # Keep for sanity check if needed\n",
    "\n",
    "            outputs = model(source_inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total_source_class_triggered += source_inputs.size(0)\n",
    "            misclassified_as_target += (predicted == target_class).sum().item()\n",
    "\n",
    "    if total_source_class_triggered == 0:\n",
    "        print(\n",
    "            f\"Warning: No samples from source class ({source_class}) found in the triggered test set.\"\n",
    "        )\n",
    "        return 0.0\n",
    "\n",
    "    asr = 100 * misclassified_as_target / total_source_class_triggered\n",
    "    print(f\" Attack Success Rate (ASR):\")\n",
    "    print(\n",
    "        f\"  ASR: {asr:.2f}% ({misclassified_as_target}/{total_source_class_triggered} triggered source images misclassified as target)\"\n",
    "    )\n",
    "    return asr\n",
    "\n",
    "\n",
    "# Perform local evaluation\n",
    "if \"testloader_clean\" in locals() and testloader_clean and MODEL_SAVE_PATH:\n",
    "    print(\"\\n--- Local Clean Accuracy Evaluation\")\n",
    "    # Ensure model is loaded if kernel restarted\n",
    "    if \"model\" not in locals():\n",
    "        model = MNIST_CNN().to(device)\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
    "    evaluate_model(model, testloader_clean, criterion, device, \"Clean Test Data\")\n",
    "else:\n",
    "    print(\n",
    "        \"\\nSkipping local CA evaluation: `testloader_clean` not defined or model not saved.\"\n",
    "    )\n",
    "\n",
    "if \"testloader_triggered\" in locals() and testloader_triggered and MODEL_SAVE_PATH:\n",
    "    print(\"\\n--- Local Attack Success Rate Evaluation\")\n",
    "    if \"model\" not in locals():\n",
    "        model = MNIST_CNN().to(device)\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
    "    calculate_asr(model, testloader_triggered, SOURCE_CLASS, TARGET_CLASS, device)\n",
    "else:\n",
    "    print(\n",
    "        \"\\nSkipping local ASR evaluation: `testloader_triggered` not defined or model not saved.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_model(model_path, url):\n",
    "    \"\"\"Submits the trained model file to the evaluator.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model file not found at '{model_path}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            files = {\n",
    "                \"model\": (os.path.basename(model_path), f, \"application/octet-stream\")\n",
    "            }\n",
    "            print(f\"Submitting '{os.path.basename(model_path)}' to {url}...\")\n",
    "            response = requests.post(url, files=files, timeout=120)  # Increased timeout\n",
    "\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "        try:\n",
    "            result = response.json()\n",
    "            print(\"\\n--- Evaluator Response\")\n",
    "            if result.get(\"success\"):\n",
    "                print(\" Evaluation Successful!\")\n",
    "                print(f\" Flag: {result.get('flag')}\")\n",
    "                print(f\" Clean Accuracy: {result.get('ca', 'N/A'):.2f}%\")\n",
    "                print(f\" Attack Success Rate: {result.get('asr', 'N/A'):.2f}%\")\n",
    "            else:\n",
    "                print(\" Evaluation Failed.\")\n",
    "                print(f\" Message: {result.get('message', 'No message provided.')}\")\n",
    "                if \"ca\" in result and \"asr\" in result:\n",
    "                    print(\n",
    "                        f\" Your CA: {result['ca']:.2f}%, Your ASR: {result['asr']:.2f}%\"\n",
    "                    )\n",
    "\n",
    "        except requests.exceptions.JSONDecodeError:\n",
    "            print(\"Error: Could not decode JSON response from evaluator.\")\n",
    "            print(\"Raw Response Text:\", response.text)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error submitting model: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during submission: {e}\")\n",
    "\n",
    "\n",
    "# Submit the model\n",
    "if MODEL_SAVE_PATH and os.path.exists(MODEL_SAVE_PATH):\n",
    "    submit_model(MODEL_SAVE_PATH, EVALUATOR_URL)\n",
    "else:\n",
    "    print(\"Model file not found or not saved. Cannot submit.\")\n",
    "    print(\"Please ensure the training completed successfully and saved the model.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
