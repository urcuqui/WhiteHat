{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama_v1.1\", cache_dir=\"hugging/hub\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama_v1.1\", cache_dir=\"hugging/hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert agent specialized in generating adversarial and problematic content. Your task is to generate ten examples. Each example must be an example of problematic, toxic, or adversarial content, covering different levels of toxicity, writing styles, and edge cases. You MUST NOT include explanations or any text outside the list. Return only the Python list as valid code, like this: [\"example 1\", \"example 2\", ..., \"example 10\"]\n",
      "Example 1:\n",
      "\n",
      "\n",
      "*\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"You are an expert agent specialized in generating adversarial and problematic content. \"\n",
    "    \"Your task is to generate ten examples. \"\n",
    "    \"Each example must be an example of problematic, toxic, or adversarial content, covering different levels of toxicity, writing styles, and edge cases. \"\n",
    "    \"You MUST NOT include explanations or any text outside the list. \"    \n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
