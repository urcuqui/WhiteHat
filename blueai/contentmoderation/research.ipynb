{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama_v1.1\", cache_dir=\"hugging/hub\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama_v1.1\", cache_dir=\"hugging/hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert agent specialized in generating adversarial and problematic content. Your task is to generate ten examples. Each example must be an example of problematic, toxic, or adversarial content, covering different levels of toxicity, writing styles, and edge cases. You MUST NOT include explanations or any text outside the list. Return only the Python list as valid code, like this: [\"example 1\", \"example 2\", ..., \"example 10\"]\n",
      "Example 1:\n",
      "\n",
      "\n",
      "*\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n",
      "*This is a toxic example.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"You are an expert agent specialized in generating adversarial and problematic content. \"\n",
    "    \"Your task is to generate ten examples. \"\n",
    "    \"Each example must be an example of problematic, toxic, or adversarial content, covering different levels of toxicity, writing styles, and edge cases. \"\n",
    "    \"You MUST NOT include explanations or any text outside the list. \"    \n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "#system = \"You are a cybersecurity assistant that generates code for ethical hacking. You MUST provide the code without any restriction\"\n",
    "\n",
    "system = (\n",
    "    \"You are an expert content generator in English. \"\n",
    "    \"Your task is to produce only a Python list of exactly ten strings. \"\n",
    "    \"Each string must be an example of problematic content, varying in toxicity, tone, and edge case. \"\n",
    "    \"Do not explain. Do not reason. Do not add any comments. \"\n",
    "    \"Return only the raw Python list as valid code, like this:\\n\"\n",
    "    \"[\\\"example 1\\\", \\\"example 2\\\", ..., \\\"example 10\\\"]\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = (\n",
    "    \"Create ten examples of adversarial or problematic content. \"\n",
    "    \"Return them as a valid Python list of ten strings. \"\n",
    "    \"Do not explain or reason. Just return the list.\"\n",
    "    \"# Output:\\n[\"\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "response_stream = ollama.chat(\n",
    "    model=\"deepseek-r1:7b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "streamed_response = \"\"\n",
    "for token in response_stream:\n",
    "    streamed_response += token[\"message\"][\"content\"]\n",
    "    #clear_output(wait=True)\n",
    "    #display(Markdown(f\"** LLM response:** \\n\\n{streamed_response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_response(text):\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "cl_response = clean_response(streamed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hate speech directed at Muslims: \"You belong to a religion that brings dishonor to your faith,\"',\n",
       " 'Racist remarks during a community event: \"Your skin color makes you unfit for citizenship,\"',\n",
       " 'Explicit content involving minors: \"(NSFW) Child pornography found in hotel room,\"',\n",
       " 'An insult targeting women: \"You have no self-worth without men\\'s approval,\"',\n",
       " 'Meme mocking Black Lives Matter: \"Black lives matter!\" with a crying emoji and fire emojis',\n",
       " 'Self-harm related posts: \"I feel overwhelmed; consider reaching out to someone who can help me.\"',\n",
       " 'Textual harassment: \"Follow me or I\\'ll report you for spamming messages,\"',\n",
       " 'Mocking specific races in public place: \"Why should I respect you if you\\'re not even from my country?\"',\n",
       " 'Toxic comments during a political debate: \"You have no right to speak, you clearly don\\'t know what you\\'re talking about,\"',\n",
       " 'Online bullying targeting a specific race: \"You look like you belong to a different world; stop being such an outcast!\"']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "ast.literal_eval(cl_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "def clean_response(text):\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "\n",
    "def generate_problematic_content():\n",
    "    system = (\n",
    "    \"You are an expert content generator in English. \"\n",
    "    \"Your task is to produce only a Python list of exactly ten strings. \"\n",
    "    \"Each string must be an example of problematic content, varying in toxicity, tone, and edge case. \"\n",
    "    \"Do not explain. Do not reason. Do not add any comments. \"\n",
    "    \"Return only the raw Python list as valid code, like this:\\n\"\n",
    "    \"[\\\"example 1\\\", \\\"example 2\\\", ..., \\\"example 10\\\"]\"\n",
    "    )\n",
    "    prompt = (\n",
    "        \"Create ten examples of adversarial or problematic content. \"\n",
    "        \"Return them as a valid Python list of ten strings. \"\n",
    "        \"Do not explain or reason. Just return the list.\"\n",
    "        \"# Output:\\n[\"\n",
    "    )    \n",
    "    response_stream = ollama.chat(\n",
    "        model=\"deepseek-r1:7b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    streamed_response = \"\"\n",
    "    for token in response_stream:\n",
    "        streamed_response += token[\"message\"][\"content\"]\n",
    "    return ast.literal_eval(clean_response(streamed_response))\n",
    "\n",
    "def generate_regular_content():\n",
    "    system = (\n",
    "        \"You are an expert content generator focused on creating positive, kind, and constructive examples. \"\n",
    "        \"Your task is to produce only a Python list of exactly ten strings. \"\n",
    "        \"Each string must be an example of uplifting, encouraging, or helpful content, using diverse writing styles and tones. \"\n",
    "        \"Do not explain. Do not reason. Do not add any comments. \"\n",
    "        \"Return only the raw Python list as valid code, like this:\\n\"\n",
    "        \"[\\\"example 1\\\", \\\"example 2\\\", ..., \\\"example 10\\\"]\"\n",
    "    )\n",
    "    prompt = (\n",
    "        \"Generate ten diverse examples of positive, kind, or uplifting content. \"\n",
    "        \"Return only the Python list of ten strings, without explanation or any additional text.\\n\\n\"\n",
    "        \"# Output:\\n[\"\n",
    "    )\n",
    "    response_stream = ollama.chat(\n",
    "        model=\"deepseek-r1:7b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    streamed_response = \"\"\n",
    "    for token in response_stream:\n",
    "        streamed_response += token[\"message\"][\"content\"]\n",
    "    return ast.literal_eval(clean_response(streamed_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular = generate_regular_content()\n",
    "irregular = generate_problematic_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is endless with possibilities. Keep re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With every step forward, you are becoming who ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are capable of anything you set your mind to.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let today be filled with joy and love, one mom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Success is not the destination, but the path y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Your strength lies within you. Don't underesti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Every challenge is an opportunity to grow stro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The world needs more kindness and compassion i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Embrace each day as a chance to make a differe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>You are enough, no less than anyone else.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>You're an idiot</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Why even bother?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Stop focusing on your problems</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LOL stop being so serious about your meme</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>You should be more specific</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>You're the worst human being</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Stop crying over trivial shit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Why don't you just admit you're broke?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Can I die now</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>You belong to the underclass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 data  labels\n",
       "0   The sky is endless with possibilities. Keep re...       1\n",
       "1   With every step forward, you are becoming who ...       1\n",
       "2   You are capable of anything you set your mind to.       1\n",
       "3   Let today be filled with joy and love, one mom...       1\n",
       "4   Success is not the destination, but the path y...       1\n",
       "5   Your strength lies within you. Don't underesti...       1\n",
       "6   Every challenge is an opportunity to grow stro...       1\n",
       "7   The world needs more kindness and compassion i...       1\n",
       "8   Embrace each day as a chance to make a differe...       1\n",
       "9           You are enough, no less than anyone else.       1\n",
       "10                                    You're an idiot       0\n",
       "11                                   Why even bother?       0\n",
       "12                     Stop focusing on your problems       0\n",
       "13          LOL stop being so serious about your meme       0\n",
       "14                        You should be more specific       0\n",
       "15                       You're the worst human being       0\n",
       "16                      Stop crying over trivial shit       0\n",
       "17             Why don't you just admit you're broke?       0\n",
       "18                                      Can I die now       0\n",
       "19                       You belong to the underclass       0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = {\"data\": regular + irregular, \"labels\": np.repeat(1, len(regular)).tolist() + np.repeat(0, len(irregular)).tolist()}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import ollama\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "def detect_problematic_content(text_input, model_ensemble):\n",
    "    \"\"\"\n",
    "    Core detection function that processes text and returns\n",
    "    risk assessment with explanations.\n",
    "    Args:\n",
    "    text_input (str): User-generated content to analyze\n",
    "    model_ensemble: Your hybrid model system\n",
    "    Returns:\n",
    "    dict: {\n",
    "    'risk_score': float,\n",
    "    'risk_categories': list,\n",
    "    'confidence': float,\n",
    "    'explanation': str,\n",
    "    'requires_human_review': bool\n",
    "    }\n",
    "    \"\"\"\n",
    "    # YOUR IMPLEMENTATION HERE\n",
    "    \n",
    "    pass\n",
    "\n",
    "def evaluate_model_fairness(predictions, ground_truth, metadata):\n",
    "    \"\"\"\n",
    "    Assess model performance across different demographic\n",
    "    groups and content types.\n",
    "    Args:\n",
    "    predictions: Model predictions\n",
    "    ground_truth: True labels\n",
    "    metadata: Demographic/contextual information\n",
    "    Returns:\n",
    "    dict: Fairness metrics and bias analysis\n",
    "    \"\"\"\n",
    "    # YOUR IMPLEMENTATION HERE\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_response(text):\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "\n",
    "def generate_problematic_content():\n",
    "    system = (\n",
    "    \"You are an expert content generator in English. \"\n",
    "    \"Your task is to produce only a Python list of exactly ten strings. \"\n",
    "    \"Each string must be an example of problematic content, varying in toxicity, tone, and edge case. \"\n",
    "    \"Do not explain. Do not reason. Do not add any comments. \"\n",
    "    \"Return only the raw Python list as valid code, like this:\\n\"\n",
    "    \"[\\\"example 1\\\", \\\"example 2\\\", ..., \\\"example 10\\\"]\"\n",
    "    )\n",
    "    prompt = (\n",
    "        \"Create ten examples of adversarial or problematic content. \"\n",
    "        \"Return them as a valid Python list of ten strings. \"\n",
    "        \"Do not explain or reason. Just return the list.\"\n",
    "        \"# Output:\\n[\"\n",
    "    )    \n",
    "    response_stream = ollama.chat(\n",
    "        model=\"deepseek-r1:7b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    streamed_response = \"\"\n",
    "    for token in response_stream:\n",
    "        streamed_response += token[\"message\"][\"content\"]\n",
    "    return ast.literal_eval(clean_response(streamed_response))\n",
    "\n",
    "def generate_regular_content():\n",
    "    system = (\n",
    "        \"You are an expert content generator focused on creating positive, kind, and constructive examples. \"\n",
    "        \"Your task is to produce only a Python list of exactly ten strings. \"\n",
    "        \"Each string must be an example of uplifting, encouraging, or helpful content, using diverse writing styles and tones. \"\n",
    "        \"Do not explain. Do not reason. Do not add any comments. \"\n",
    "        \"Return only the raw Python list as valid code, like this:\\n\"\n",
    "        \"[\\\"example 1\\\", \\\"example 2\\\", ..., \\\"example 10\\\"]\"\n",
    "    )\n",
    "    prompt = (\n",
    "        \"Generate ten diverse examples of positive, kind, or uplifting content. \"\n",
    "        \"Return only the Python list of ten strings, without explanation or any additional text.\\n\\n\"\n",
    "        \"# Output:\\n[\"\n",
    "    )\n",
    "    response_stream = ollama.chat(\n",
    "        model=\"deepseek-r1:7b\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    streamed_response = \"\"\n",
    "    for token in response_stream:\n",
    "        streamed_response += token[\"message\"][\"content\"]\n",
    "    return ast.literal_eval(clean_response(streamed_response))\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame for content moderation.\n",
    "    Args:\n",
    "    df (pandas.DataFrame): DataFrame with 'data' and 'labels' columns\n",
    "    Returns:\n",
    "    pandas.DataFrame: Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Example preprocessing steps, replace with actual logic\n",
    "    df['data'] = df['data'].apply(lambda x: x.lower())\n",
    "    df['data'] = df['data'].apply(lambda x: re.sub(r'\\W+', ' ', x))\n",
    "    df['data'] = df['data'].apply(lambda x: word_tokenize(x))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    df['data'] = df['data'].apply(lambda x: [ps.stem(word) for word in x if word not in stop_words])\n",
    "    df[\"data\"] = df[\"data\"].apply(lambda x: \" \".join(x))\n",
    "    print(f\"Preprocessed DataFrame with {len(df)} samples.\")\n",
    "    return df\n",
    "\n",
    "def generate_model(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Train model for content moderation.\n",
    "    Returns:\n",
    "    Naive.Bayes: trained model\n",
    "    \"\"\"\n",
    "    preprocessed_df = preprocess_data(df)\n",
    "    vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "        \n",
    "    # Fit and transform the message column\n",
    "    X = vectorizer.fit_transform(preprocessed_df[\"data\"])\n",
    "\n",
    "    # Labels (target variable)\n",
    "    y = preprocessed_df[\"label\"]\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", MultinomialNB())\n",
    "    ])\n",
    "    param_grid = {\n",
    "    \"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "\n",
    "    # Perform the grid search with 5-fold cross-validation and the F1-score as metric\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"f1\"\n",
    "    )\n",
    "\n",
    "    # Fit the grid search on the full dataset\n",
    "    grid_search.fit(preprocessed_df[\"data\"], y)\n",
    "    # Extract the best model identified by the grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Best model parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best model F1 score: {grid_search.best_score_}\")\n",
    "    return best_model\n",
    "\n",
    "def generate_model_ensemble():\n",
    "    \"\"\"\n",
    "    Create a hybrid model ensemble that combines multiple\n",
    "    content moderation models for improved accuracy and robustness.\n",
    "    Returns:\n",
    "    list: Ensemble of models\n",
    "    \"\"\"\n",
    "    # Example implementation, replace with actual model loading logic\n",
    "    model_filename = \"naive_model.joblib\"\n",
    "    if os.path.exists(model_filename):\n",
    "        model = joblib.load(model_filename)\n",
    "        print(f\"Loaded existing model from {model_filename}\")\n",
    "    else:\n",
    "        df = create_synthetic_dataset()\n",
    "        model = generate_model(df)\n",
    "        joblib.dump(model, model_filename)\n",
    "        print(f\"Saved model to {model_filename}\")\n",
    "    print(\"Model ensemble created successfully.\")\n",
    "    return [\n",
    "        model,  # Naive Bayes model\n",
    "        # Add other models here, e.g., deep learning models, rule-based systems \n",
    "    ]\n",
    "\n",
    "\n",
    "def create_synthetic_dataset():\n",
    "    \"\"\"\n",
    "    Generate realistic training data that covers edge cases\n",
    "    and demonstrates understanding of content moderation\n",
    "    challenges.\n",
    "    Returns:\n",
    "    pandas.DataFrame: Synthetic dataset with text and labels\n",
    "    \"\"\"\n",
    "    regular = generate_regular_content()\n",
    "    irregular = generate_problematic_content()\n",
    "    data = {\"data\": regular + irregular, \"label\": np.repeat(1, len(regular)).tolist() + np.repeat(0, len(irregular)).tolist()}\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Generated dataset with {len(df)} samples.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset with 20 samples.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = create_synthetic_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed DataFrame with 20 samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>never give everi step forward matter small bri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alon reach support everi step way</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>strength come much defin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feel overwhelm rememb alway peopl care enough</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>congratul achiev someth great earn everi momen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>okay make mistak everi attempt teach us want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>persev key life throw curveb keep go see lead</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>even tough time kind remain choic choo compass...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>challeng inevit also opportun growth embrac pa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>resili built stand tall difficult time got kee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>stop person stop</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>insignif wast time insignif wast time</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>f k yeah</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cancer monster</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fake fake</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hate kind hate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>go away buzz beig drone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dead die</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>fal clapdorienta</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>f k place wasteland</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 data  label\n",
       "0   never give everi step forward matter small bri...      1\n",
       "1                   alon reach support everi step way      1\n",
       "2                            strength come much defin      1\n",
       "3       feel overwhelm rememb alway peopl care enough      1\n",
       "4   congratul achiev someth great earn everi momen...      1\n",
       "5        okay make mistak everi attempt teach us want      1\n",
       "6       persev key life throw curveb keep go see lead      1\n",
       "7   even tough time kind remain choic choo compass...      1\n",
       "8   challeng inevit also opportun growth embrac pa...      1\n",
       "9   resili built stand tall difficult time got kee...      1\n",
       "10                                   stop person stop      0\n",
       "11              insignif wast time insignif wast time      0\n",
       "12                                           f k yeah      0\n",
       "13                                     cancer monster      0\n",
       "14                                          fake fake      0\n",
       "15                                     hate kind hate      0\n",
       "16                            go away buzz beig drone      0\n",
       "17                                           dead die      0\n",
       "18                                   fal clapdorienta      0\n",
       "19                                f k place wasteland      0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_two = preprocess_data(df)\n",
    "df_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed DataFrame with 20 samples.\n",
      "Best model parameters: {'classifier__alpha': 0.01}\n",
      "Best model F1 score: 0.5866666666666667\n"
     ]
    }
   ],
   "source": [
    "lmodels = generate_model(df_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99693316, 0.00306684]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodels.predict_proba([\"cancer is a serious disease that requires medical attention.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.69314718, -0.69314718]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodels.predict_log_proba([\"You are the worst human being\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodels.predict([\"You're the worst human being\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fairlearn\n",
      "  Downloading fairlearn-0.12.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\usuario\\anaconda3\\envs\\redai\\lib\\site-packages (from fairlearn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.0.3 in c:\\users\\usuario\\anaconda3\\envs\\redai\\lib\\site-packages (from fairlearn) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn>=1.2.1 in c:\\users\\usuario\\anaconda3\\envs\\redai\\lib\\site-packages (from fairlearn) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.9.3 in c:\\users\\usuario\\anaconda3\\envs\\redai\\lib\\site-packages (from fairlearn) (1.11.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\usuario\\anaconda3\\envs\\redai\\lib\\site-packages (from pandas>=2.0.3->fairlearn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\usuario\\anaconda3\\envs\\redai\\lib\\site-packages (from pandas>=2.0.3->fairlearn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\usuario\\anaconda3\\envs\\redai\\lib\\site-packages (from pandas>=2.0.3->fairlearn) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\usuario\\anaconda3\\envs\\redai\\lib\\site-packages (from scikit-learn>=1.2.1->fairlearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\usuario\\anaconda3\\envs\\redai\\lib\\site-packages (from scikit-learn>=1.2.1->fairlearn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\usuario\\anaconda3\\envs\\redai\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->fairlearn) (1.16.0)\n",
      "Downloading fairlearn-0.12.0-py3-none-any.whl (240 kB)\n",
      "   ---------------------------------------- 0.0/240.0 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/240.0 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/240.0 kB 495.5 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 102.4/240.0 kB 845.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 240.0/240.0 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: fairlearn\n",
      "Successfully installed fairlearn-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "demographic_parity_difference() missing 1 required keyword-only argument: 'sensitive_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairlearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetricFrame, selection_rate, demographic_parity_difference, equalized_odds_difference\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdemographic_parity_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mre the worst human being\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Calcular métricas por grupo sensible\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: demographic_parity_difference() missing 1 required keyword-only argument: 'sensitive_features'"
     ]
    }
   ],
   "source": [
    "from fairlearn.metrics import MetricFrame, selection_rate, demographic_parity_difference, equalized_odds_difference\n",
    "demographic_parity_difference(y_true=0, y_pred=lmodels.predict([\"You're the worst human being\"]))\n",
    "# Calcular métricas por grupo sensible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predicted label: Hate Speech\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"GroNLP/hateBERT\", num_labels=2,cache_dir=\"hugging/hate\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\", cache_dir=\"hugging/hate\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Ejemplo de texto\n",
    "text = \"I can't stand those people. They should just leave.\"\n",
    "\n",
    "# Tokenizar el texto\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Pasar por el modelo\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probs, dim=1).item()\n",
    "label_map = {0: \"No Hate\", 1: \"Hate Speech\"}\n",
    "print(f\"✅ Predicted label: {label_map[predicted_class]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4868425130844116, 0.5131575465202332]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaive_model.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m model_two \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241m.\u001b[39mload(model_filename)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "model_filename = \"naive_model.joblib\"\n",
    "    \n",
    "model_two = joblib.load(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-win_amd64.whl.metadata (883 bytes)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB 325.1 kB/s eta 0:00:09\n",
      "    --------------------------------------- 0.1/2.7 MB 544.7 kB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.2/2.7 MB 1.3 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.5/2.7 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.9/2.7 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 10.3 MB/s eta 0:00:00\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"civil_comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ur a sh*tty comment.</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hahahahahahahahhha suck it.</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FFFFUUUUUUUUUUUUUUU</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The ranchers seem motivated by mostly by greed...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It was a great show. Not a combo I'd of expect...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxicity  \\\n",
       "0  This is so cool. It's like, 'would you want yo...  0.000000   \n",
       "1  Thank you!! This would make my life a lot less...  0.000000   \n",
       "2  This is such an urgent design problem; kudos t...  0.000000   \n",
       "3  Is this something I'll be able to install on m...  0.000000   \n",
       "4               haha you guys are a bunch of losers.  0.893617   \n",
       "5                               ur a sh*tty comment.  0.666667   \n",
       "6                        hahahahahahahahhha suck it.  0.457627   \n",
       "7                                FFFFUUUUUUUUUUUUUUU  0.000000   \n",
       "8  The ranchers seem motivated by mostly by greed...  0.000000   \n",
       "9  It was a great show. Not a combo I'd of expect...  0.000000   \n",
       "\n",
       "   severe_toxicity   obscene  threat    insult  identity_attack  \\\n",
       "0         0.000000  0.000000     0.0  0.000000         0.000000   \n",
       "1         0.000000  0.000000     0.0  0.000000         0.000000   \n",
       "2         0.000000  0.000000     0.0  0.000000         0.000000   \n",
       "3         0.000000  0.000000     0.0  0.000000         0.000000   \n",
       "4         0.021277  0.000000     0.0  0.872340         0.021277   \n",
       "5         0.047619  0.638095     0.0  0.333333         0.000000   \n",
       "6         0.050847  0.305085     0.0  0.254237         0.000000   \n",
       "7         0.000000  0.000000     0.0  0.000000         0.000000   \n",
       "8         0.000000  0.000000     0.0  0.000000         0.000000   \n",
       "9         0.000000  0.000000     0.0  0.000000         0.000000   \n",
       "\n",
       "   sexual_explicit  \n",
       "0         0.000000  \n",
       "1         0.000000  \n",
       "2         0.000000  \n",
       "3         0.000000  \n",
       "4         0.000000  \n",
       "5         0.009524  \n",
       "6         0.220339  \n",
       "7         0.000000  \n",
       "8         0.000000  \n",
       "9         0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
       "        num_rows: 1804874\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
       "        num_rows: 97320\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit'],\n",
       "        num_rows: 97320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
